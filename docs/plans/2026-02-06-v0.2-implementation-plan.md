# LocalPush v0.2 Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Transform LocalPush from a single-source webhook sender into a multi-source platform with n8n target integration, ntfy target, Claude Code Sessions source, three Apple data sources, and radical transparency at every layer.

**Architecture:** Three-layer model — Target Systems (connect to n8n/ntfy), Sources (discover + preview), Source-to-Endpoint Binding (security coaching). Existing trait-based DI pattern extended with new `Target` trait alongside existing `Source` trait.

**Tech Stack:** Rust (Tauri 2.0), React 18 + TypeScript, SQLite WAL, reqwest, rusqlite, JXA/osascript, macOS Keychain.

**Design Doc:** `docs/plans/2026-02-05-v0.2-transparency-n8n-sources-design.md`
**Research:** `docs/research/research-*.md` (5 files)

---

## Execution Context

| Field | Value |
|-------|-------|
| **Working Directory** | `/Users/madsnissen/dev/localpush` |
| **Git Branch** | Create worktree: `.trees/v0.2` on branch `feature/v0.2-multi-source` |
| **Repository Root** | `/Users/madsnissen/dev/localpush` |

---

## Dependency Decision: ntfy

**Decision:** Roll our own ntfy client (simple HTTP POST) rather than adding `ntfy` crate dependency.

**Rationale:**
- ntfy publish API is trivially simple: `POST https://server/topic` with JSON body
- The `shadowylab/ntfy` crate adds 3 extra dependencies (url, reqwest builder pattern) for something that's 15 lines of code
- We already have `reqwest` — just POST JSON with headers
- ntfy fields: `topic`, `message`, `title`, `tags`, `priority`, `click`, `icon`, `markdown`
- Our existing `WebhookClient` trait already handles the HTTP layer

---

## Task Dependency Graph

```
[1] Target trait + TargetManager ──┐
                                   ├──► [3] n8n Target ──┐
[2] ntfy Target struct ────────────┘                     ├──► [6] Source-to-Target Binding
                                                         │
[4] Claude Sessions Source ─────────────────────────────►├──► [7] Frontend: Target Setup
                                                         │
[5] Apple Sources (Podcasts, Notes, Photos) ────────────►├──► [8] Frontend: Source Cards
                                                         │
                                                         └──► [9] Frontend: Activity Log
[10] Traffic Light + Tray Icons (independent)
[11] Frontend Polish + Integration
```

**Parallelizable groups:**
- Group A: Tasks 1-2 (Target infrastructure) — sequential
- Group B: Task 4 (Claude Sessions) — independent
- Group C: Task 5 (Apple sources) — independent, internally parallel (3 sub-tasks)
- Group D: Tasks 7-9 (Frontend) — depends on backend tasks
- Group E: Task 10 (Tray icons) — independent

---

## Task 1: Target Trait + TargetManager

**Purpose:** Define the abstraction for target systems (n8n, ntfy, raw webhook) and a manager to register/configure them.

**Files:**
- Create: `src-tauri/src/traits/target.rs`
- Create: `src-tauri/src/target_manager.rs`
- Modify: `src-tauri/src/traits/mod.rs`
- Modify: `src-tauri/src/state.rs`
- Test: `src-tauri/src/target_manager.rs` (inline tests)

### Step 1: Write the Target trait

Create `src-tauri/src/traits/target.rs`:

```rust
//! Target trait for delivery endpoints

use serde::{Deserialize, Serialize};
use thiserror::Error;

#[derive(Debug, Clone, Error)]
pub enum TargetError {
    #[error("Connection failed: {0}")]
    ConnectionFailed(String),
    #[error("Authentication failed: {0}")]
    AuthFailed(String),
    #[error("Invalid configuration: {0}")]
    InvalidConfig(String),
    #[error("Not connected")]
    NotConnected,
}

/// Information about a connected target for display
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TargetInfo {
    pub id: String,
    pub name: String,
    pub target_type: String,       // "n8n", "ntfy", "webhook"
    pub base_url: String,
    pub connected: bool,
    pub details: serde_json::Value, // target-specific metadata (e.g. n8n version)
}

/// An available endpoint within a target (e.g. an n8n webhook workflow)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TargetEndpoint {
    pub id: String,
    pub name: String,
    pub url: String,
    pub authenticated: bool,
    pub auth_type: Option<String>,  // "none", "headerAuth", "basicAuth"
    pub metadata: serde_json::Value,
}

/// Trait for target systems that receive data from sources
#[async_trait::async_trait]
pub trait Target: Send + Sync {
    /// Unique ID for this target instance
    fn id(&self) -> &str;

    /// Human-readable name
    fn name(&self) -> &str;

    /// Target type identifier
    fn target_type(&self) -> &str;

    /// Test connection and return target info
    async fn test_connection(&self) -> Result<TargetInfo, TargetError>;

    /// List available endpoints (e.g. webhook workflows)
    async fn list_endpoints(&self) -> Result<Vec<TargetEndpoint>, TargetError>;

    /// Get the base URL
    fn base_url(&self) -> &str;
}
```

### Step 2: Export from traits/mod.rs

Add to `src-tauri/src/traits/mod.rs`:

```rust
mod target;
pub use target::{Target, TargetError, TargetInfo, TargetEndpoint};
```

### Step 3: Create TargetManager

Create `src-tauri/src/target_manager.rs`:

```rust
//! Target Manager - Registry for delivery target systems

use std::collections::HashMap;
use std::sync::{Arc, Mutex};

use crate::config::AppConfig;
use crate::traits::{Target, TargetError, TargetInfo, TargetEndpoint};

#[derive(Debug, thiserror::Error)]
pub enum TargetManagerError {
    #[error("Target not found: {0}")]
    NotFound(String),
    #[error("Target error: {0}")]
    TargetError(#[from] TargetError),
}

pub struct TargetManager {
    targets: Mutex<HashMap<String, Arc<dyn Target>>>,
    config: Arc<AppConfig>,
}

impl TargetManager {
    pub fn new(config: Arc<AppConfig>) -> Self {
        Self {
            targets: Mutex::new(HashMap::new()),
            config,
        }
    }

    pub fn register(&self, target: Arc<dyn Target>) {
        let id = target.id().to_string();
        self.targets.lock().unwrap().insert(id, target);
    }

    pub fn get(&self, id: &str) -> Option<Arc<dyn Target>> {
        self.targets.lock().unwrap().get(id).cloned()
    }

    pub fn list(&self) -> Vec<(String, String, String)> {
        // Returns (id, name, target_type) tuples
        self.targets
            .lock()
            .unwrap()
            .iter()
            .map(|(id, t)| (id.clone(), t.name().to_string(), t.target_type().to_string()))
            .collect()
    }

    pub async fn test_connection(&self, id: &str) -> Result<TargetInfo, TargetManagerError> {
        let target = self.get(id).ok_or_else(|| TargetManagerError::NotFound(id.to_string()))?;
        Ok(target.test_connection().await?)
    }

    pub async fn list_endpoints(&self, id: &str) -> Result<Vec<TargetEndpoint>, TargetManagerError> {
        let target = self.get(id).ok_or_else(|| TargetManagerError::NotFound(id.to_string()))?;
        Ok(target.list_endpoints().await?)
    }
}
```

### Step 4: Wire into AppState

Add `target_manager` field to `AppState` in `src-tauri/src/state.rs`:

```rust
pub target_manager: Arc<TargetManager>,
```

Initialize in `new_production()`:

```rust
let target_manager = Arc::new(TargetManager::new(config.clone()));
```

### Step 5: Add Tauri commands for targets

Add to `src-tauri/src/commands/mod.rs`:

```rust
#[tauri::command]
pub async fn list_targets(state: State<'_, AppState>) -> Result<Vec<serde_json::Value>, String> { ... }

#[tauri::command]
pub async fn test_target_connection(state: State<'_, AppState>, target_id: String) -> Result<serde_json::Value, String> { ... }

#[tauri::command]
pub async fn list_target_endpoints(state: State<'_, AppState>, target_id: String) -> Result<Vec<serde_json::Value>, String> { ... }
```

### Step 6: Write tests

Inline tests in `target_manager.rs`:
- `test_register_and_list` — register a mock target, verify list returns it
- `test_get_nonexistent` — returns None
- `test_test_connection` — mock target returns TargetInfo

### Step 7: Run tests and commit

```bash
cargo test target_manager
cargo clippy -- -D warnings
git add src-tauri/src/traits/target.rs src-tauri/src/target_manager.rs src-tauri/src/traits/mod.rs src-tauri/src/state.rs src-tauri/src/commands/mod.rs
git commit -m "feat(targets): add Target trait and TargetManager"
```

---

## Task 2: ntfy Target Implementation

**Purpose:** Implement ntfy as a target system. ntfy is dead simple — HTTP POST to `{server}/{topic}` with optional auth.

**Files:**
- Create: `src-tauri/src/targets/mod.rs`
- Create: `src-tauri/src/targets/ntfy.rs`
- Modify: `src-tauri/src/state.rs` (register ntfy target)
- Modify: `src-tauri/src/lib.rs` (add `mod targets`)

### Step 1: Create targets module

Create `src-tauri/src/targets/mod.rs`:

```rust
pub mod ntfy;
pub use ntfy::NtfyTarget;
```

Add `mod targets;` to `src-tauri/src/lib.rs`.

### Step 2: Implement NtfyTarget

Create `src-tauri/src/targets/ntfy.rs`:

```rust
//! ntfy target - sends notifications to ntfy.sh or self-hosted ntfy servers
//!
//! API: POST https://{server}/{topic}
//! Body: JSON with message, title, tags, priority, etc.
//! Auth: Bearer token or basic auth (optional)
//! Docs: https://docs.ntfy.sh/publish/

use crate::traits::{Target, TargetError, TargetInfo, TargetEndpoint};
use reqwest::Client;

pub struct NtfyTarget {
    id: String,
    server_url: String,     // e.g. "https://ntfy.sh" or "https://ntfy.example.com"
    default_topic: Option<String>,
    auth_token: Option<String>,
    client: Client,
}

impl NtfyTarget {
    pub fn new(id: String, server_url: String) -> Self {
        Self {
            id,
            server_url: server_url.trim_end_matches('/').to_string(),
            default_topic: None,
            auth_token: None,
            client: Client::new(),
        }
    }

    pub fn with_topic(mut self, topic: String) -> Self {
        self.default_topic = Some(topic);
        self
    }

    pub fn with_auth(mut self, token: String) -> Self {
        self.auth_token = Some(token);
        self
    }

    /// Send a notification to a topic
    pub async fn publish(
        &self,
        topic: &str,
        title: &str,
        message: &str,
        tags: Option<Vec<String>>,
        priority: Option<u8>,
    ) -> Result<(), TargetError> {
        let url = format!("{}/{}", self.server_url, topic);

        let mut payload = serde_json::json!({
            "topic": topic,
            "message": message,
            "title": title,
        });

        if let Some(tags) = tags {
            payload["tags"] = serde_json::json!(tags);
        }
        if let Some(priority) = priority {
            payload["priority"] = serde_json::json!(priority);
        }

        let mut req = self.client.post(&url).json(&payload);

        if let Some(ref token) = self.auth_token {
            req = req.bearer_auth(token);
        }

        let response = req.send().await.map_err(|e| TargetError::ConnectionFailed(e.to_string()))?;

        if !response.status().is_success() {
            return Err(TargetError::ConnectionFailed(
                format!("HTTP {}: {}", response.status(), response.text().await.unwrap_or_default())
            ));
        }

        Ok(())
    }
}

#[async_trait::async_trait]
impl Target for NtfyTarget {
    fn id(&self) -> &str { &self.id }
    fn name(&self) -> &str { "ntfy" }
    fn target_type(&self) -> &str { "ntfy" }
    fn base_url(&self) -> &str { &self.server_url }

    async fn test_connection(&self) -> Result<TargetInfo, TargetError> {
        // ntfy health check: GET /v1/health
        let url = format!("{}/v1/health", self.server_url);
        let resp = self.client.get(&url).send().await
            .map_err(|e| TargetError::ConnectionFailed(e.to_string()))?;

        let healthy = resp.status().is_success();

        Ok(TargetInfo {
            id: self.id.clone(),
            name: "ntfy".to_string(),
            target_type: "ntfy".to_string(),
            base_url: self.server_url.clone(),
            connected: healthy,
            details: serde_json::json!({
                "healthy": healthy,
            }),
        })
    }

    async fn list_endpoints(&self) -> Result<Vec<TargetEndpoint>, TargetError> {
        // ntfy doesn't have discoverable endpoints — user specifies topic
        // If a default topic is set, return it as the single endpoint
        let mut endpoints = Vec::new();
        if let Some(ref topic) = self.default_topic {
            endpoints.push(TargetEndpoint {
                id: topic.clone(),
                name: format!("Topic: {}", topic),
                url: format!("{}/{}", self.server_url, topic),
                authenticated: self.auth_token.is_some(),
                auth_type: if self.auth_token.is_some() { Some("bearer".to_string()) } else { None },
                metadata: serde_json::json!({}),
            });
        }
        Ok(endpoints)
    }
}
```

### Step 3: Write tests

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_ntfy_target_creation() {
        let target = NtfyTarget::new("ntfy-1".to_string(), "https://ntfy.sh".to_string());
        assert_eq!(target.id(), "ntfy-1");
        assert_eq!(target.target_type(), "ntfy");
        assert_eq!(target.base_url(), "https://ntfy.sh");
    }

    #[test]
    fn test_ntfy_with_topic() {
        let target = NtfyTarget::new("ntfy-1".to_string(), "https://ntfy.sh".to_string())
            .with_topic("localpush-alerts".to_string());
        assert_eq!(target.default_topic.as_deref(), Some("localpush-alerts"));
    }

    #[test]
    fn test_trailing_slash_stripped() {
        let target = NtfyTarget::new("t".to_string(), "https://ntfy.sh/".to_string());
        assert_eq!(target.base_url(), "https://ntfy.sh");
    }
}
```

### Step 4: Run tests and commit

```bash
cargo test ntfy
cargo clippy -- -D warnings
git add src-tauri/src/targets/
git commit -m "feat(targets): add ntfy target implementation"
```

---

## Task 3: n8n Target Implementation

**Purpose:** Implement n8n as a first-class target. Connect with URL + API key, discover active webhook workflows, extract endpoint URLs and auth status.

**Files:**
- Create: `src-tauri/src/targets/n8n.rs`
- Modify: `src-tauri/src/targets/mod.rs`
- Modify: `src-tauri/src/state.rs`

**Reference:** `docs/research/research-n8n-api.md`

### Step 1: Implement N8nTarget

Create `src-tauri/src/targets/n8n.rs`:

```rust
//! n8n target - connects to an n8n instance and discovers webhook workflows
//!
//! API: GET /api/v1/workflows?active=true (list)
//!      GET /api/v1/workflows/{id} (full details with nodes)
//! Auth: X-N8N-API-KEY header
//!
//! Reference: docs/research/research-n8n-api.md

use crate::traits::{Target, TargetError, TargetInfo, TargetEndpoint};
use reqwest::Client;
use serde::Deserialize;

pub struct N8nTarget {
    id: String,
    instance_url: String,  // e.g. "https://flow.rightaim.ai"
    api_key: String,
    client: Client,
}

#[derive(Deserialize)]
struct WorkflowListResponse {
    workflows: Vec<WorkflowSummary>,
    #[serde(rename = "hasMore")]
    has_more: bool,
    #[serde(rename = "nextCursor")]
    next_cursor: Option<String>,
}

#[derive(Deserialize)]
struct WorkflowSummary {
    id: String,
    name: String,
    active: bool,
    #[serde(rename = "nodeCount")]
    node_count: Option<u32>,
}

#[derive(Deserialize)]
struct WorkflowFull {
    id: String,
    name: String,
    active: bool,
    nodes: Vec<WorkflowNode>,
}

#[derive(Deserialize)]
struct WorkflowNode {
    id: Option<String>,
    name: String,
    #[serde(rename = "type")]
    node_type: String,
    #[serde(rename = "webhookId")]
    webhook_id: Option<String>,
    parameters: Option<serde_json::Value>,
    credentials: Option<serde_json::Value>,
}

impl N8nTarget {
    pub fn new(id: String, instance_url: String, api_key: String) -> Self {
        Self {
            id,
            instance_url: instance_url.trim_end_matches('/').to_string(),
            api_key,
            client: Client::new(),
        }
    }

    fn api_url(&self, path: &str) -> String {
        format!("{}/api/v1{}", self.instance_url, path)
    }

    async fn fetch_workflows(&self) -> Result<Vec<WorkflowSummary>, TargetError> {
        let mut all_workflows = Vec::new();
        let mut cursor: Option<String> = None;

        loop {
            let mut url = format!("{}?active=true&limit=100", self.api_url("/workflows"));
            if let Some(ref c) = cursor {
                url.push_str(&format!("&cursor={}", c));
            }

            let resp = self.client.get(&url)
                .header("X-N8N-API-KEY", &self.api_key)
                .send()
                .await
                .map_err(|e| TargetError::ConnectionFailed(e.to_string()))?;

            if resp.status() == 401 || resp.status() == 403 {
                return Err(TargetError::AuthFailed("Invalid API key".to_string()));
            }

            if !resp.status().is_success() {
                return Err(TargetError::ConnectionFailed(
                    format!("HTTP {}", resp.status())
                ));
            }

            let body: WorkflowListResponse = resp.json().await
                .map_err(|e| TargetError::ConnectionFailed(e.to_string()))?;

            all_workflows.extend(body.workflows);

            if !body.has_more {
                break;
            }
            cursor = body.next_cursor;
        }

        Ok(all_workflows)
    }

    async fn fetch_workflow_details(&self, workflow_id: &str) -> Result<WorkflowFull, TargetError> {
        let url = self.api_url(&format!("/workflows/{}", workflow_id));
        let resp = self.client.get(&url)
            .header("X-N8N-API-KEY", &self.api_key)
            .send()
            .await
            .map_err(|e| TargetError::ConnectionFailed(e.to_string()))?;

        if !resp.status().is_success() {
            return Err(TargetError::ConnectionFailed(format!("HTTP {}", resp.status())));
        }

        resp.json().await
            .map_err(|e| TargetError::ConnectionFailed(e.to_string()))
    }

    fn extract_webhook_endpoints(&self, workflow: &WorkflowFull) -> Vec<TargetEndpoint> {
        workflow.nodes.iter()
            .filter(|n| n.node_type == "n8n-nodes-base.webhook")
            .filter_map(|node| {
                let params = node.parameters.as_ref()?;
                let path = params.get("path")?.as_str()?;
                let auth = params.get("authentication")
                    .and_then(|a| a.as_str())
                    .unwrap_or("none");

                let webhook_url = format!("{}/webhook/{}", self.instance_url, path);

                Some(TargetEndpoint {
                    id: format!("{}:{}", workflow.id, node.name),
                    name: format!("{} > {}", workflow.name, node.name),
                    url: webhook_url,
                    authenticated: auth != "none",
                    auth_type: Some(auth.to_string()),
                    metadata: serde_json::json!({
                        "workflow_id": workflow.id,
                        "workflow_name": workflow.name,
                        "node_name": node.name,
                        "http_method": params.get("httpMethod").and_then(|m| m.as_str()).unwrap_or("POST"),
                        "webhook_id": node.webhook_id,
                    }),
                })
            })
            .collect()
    }
}

#[async_trait::async_trait]
impl Target for N8nTarget {
    fn id(&self) -> &str { &self.id }
    fn name(&self) -> &str { "n8n" }
    fn target_type(&self) -> &str { "n8n" }
    fn base_url(&self) -> &str { &self.instance_url }

    async fn test_connection(&self) -> Result<TargetInfo, TargetError> {
        let workflows = self.fetch_workflows().await?;

        Ok(TargetInfo {
            id: self.id.clone(),
            name: "n8n".to_string(),
            target_type: "n8n".to_string(),
            base_url: self.instance_url.clone(),
            connected: true,
            details: serde_json::json!({
                "active_workflows": workflows.len(),
            }),
        })
    }

    async fn list_endpoints(&self) -> Result<Vec<TargetEndpoint>, TargetError> {
        let workflows = self.fetch_workflows().await?;
        let mut endpoints = Vec::new();

        for wf in &workflows {
            let full = self.fetch_workflow_details(&wf.id).await?;
            endpoints.extend(self.extract_webhook_endpoints(&full));
        }

        Ok(endpoints)
    }
}
```

### Step 2: Add to targets/mod.rs

```rust
pub mod n8n;
pub use n8n::N8nTarget;
```

### Step 3: Write tests

Test webhook node detection with mock JSON:

```rust
#[cfg(test)]
mod tests {
    use super::*;

    fn mock_workflow() -> WorkflowFull {
        WorkflowFull {
            id: "abc123".to_string(),
            name: "Test Workflow".to_string(),
            active: true,
            nodes: vec![
                WorkflowNode {
                    id: Some("webhook-1".to_string()),
                    name: "Analytics Webhook".to_string(),
                    node_type: "n8n-nodes-base.webhook".to_string(),
                    webhook_id: Some("a8e3-uuid".to_string()),
                    parameters: Some(serde_json::json!({
                        "path": "analytics",
                        "httpMethod": "POST",
                        "authentication": "none",
                    })),
                    credentials: None,
                },
                WorkflowNode {
                    id: Some("code-1".to_string()),
                    name: "Process Data".to_string(),
                    node_type: "n8n-nodes-base.code".to_string(),
                    webhook_id: None,
                    parameters: None,
                    credentials: None,
                },
            ],
        }
    }

    #[test]
    fn test_extract_webhook_endpoints() {
        let target = N8nTarget::new(
            "n8n-1".to_string(),
            "https://flow.example.com".to_string(),
            "fake-key".to_string(),
        );
        let wf = mock_workflow();
        let endpoints = target.extract_webhook_endpoints(&wf);

        assert_eq!(endpoints.len(), 1);
        assert_eq!(endpoints[0].url, "https://flow.example.com/webhook/analytics");
        assert!(!endpoints[0].authenticated);
        assert_eq!(endpoints[0].name, "Test Workflow > Analytics Webhook");
    }

    #[test]
    fn test_authenticated_webhook_detection() {
        let target = N8nTarget::new(
            "n8n-1".to_string(),
            "https://flow.example.com".to_string(),
            "fake-key".to_string(),
        );
        let mut wf = mock_workflow();
        wf.nodes[0].parameters = Some(serde_json::json!({
            "path": "secure",
            "httpMethod": "POST",
            "authentication": "headerAuth",
        }));
        let endpoints = target.extract_webhook_endpoints(&wf);

        assert_eq!(endpoints.len(), 1);
        assert!(endpoints[0].authenticated);
        assert_eq!(endpoints[0].auth_type.as_deref(), Some("headerAuth"));
    }
}
```

### Step 4: Add Tauri commands for n8n connection flow

Add to `src-tauri/src/commands/mod.rs`:

```rust
/// Connect an n8n target (URL + API key)
#[tauri::command]
pub async fn connect_n8n_target(
    state: State<'_, AppState>,
    instance_url: String,
    api_key: String,
) -> Result<serde_json::Value, String> { ... }
```

### Step 5: Run tests and commit

```bash
cargo test n8n
cargo clippy -- -D warnings
git add src-tauri/src/targets/n8n.rs src-tauri/src/targets/mod.rs
git commit -m "feat(targets): add n8n target with webhook discovery"
```

---

## Task 4: Claude Code Sessions Source

**Purpose:** New source that captures Claude Code session activity (start/end, duration, tokens, topic, project). Watches `sessions-index.json` files, parses JSONL for token details.

**Files:**
- Create: `src-tauri/src/sources/claude_sessions.rs`
- Modify: `src-tauri/src/sources/mod.rs`
- Modify: `src-tauri/src/state.rs` (register source)

**Reference:** `docs/research/research-claude-sessions.md`

### Step 1: Create ClaudeSessionsSource

Create `src-tauri/src/sources/claude_sessions.rs`:

Key design decisions:
- Watch `~/.claude/projects/*/sessions-index.json` — use the parent directory `~/.claude/projects/` as watch path
- On change: read all `sessions-index.json` files, find recently modified sessions
- For each new/modified session: parse the corresponding JSONL file for token counts
- Track last-seen session timestamps to only report new/changed sessions

**Struct:**
```rust
pub struct ClaudeSessionsSource {
    claude_dir: PathBuf,        // ~/.claude/projects/
    last_seen: Mutex<HashMap<String, i64>>,  // session_id -> last modified timestamp
}
```

**Source trait implementation:**
- `id()` → `"claude-sessions"`
- `name()` → `"Claude Code Sessions"`
- `watch_path()` → `~/.claude/projects/` (watch directory for any sessions-index.json changes)
- `parse()` → scan all project dirs, read sessions-index.json, identify new sessions, parse JSONL for tokens, return payload
- `preview()` → show most recent session title, total sessions today, total tokens today

**Payload structure:**
```json
{
  "source": "claude_code_sessions",
  "sessions": [
    {
      "id": "uuid",
      "project_path": "/Users/mads/dev/localpush",
      "git_branch": "main",
      "title": "v0.2 planning session",
      "start_time": "2026-02-05T21:44:48Z",
      "end_time": "2026-02-06T10:05:16Z",
      "duration_seconds": 44428,
      "message_count": 47,
      "tokens": { "input": 12589, "output": 8234, "cache_read": 456789, "cache_creation": 23456 },
      "model": "claude-opus-4-6"
    }
  ],
  "summary": {
    "sessions_today": 3,
    "total_tokens_today": 125000,
    "total_duration_today_seconds": 7200
  }
}
```

### Step 2: Implement sessions-index.json parsing

Parse the index file format:
```rust
#[derive(Deserialize)]
struct SessionIndexEntry {
    #[serde(rename = "sessionId")]
    session_id: String,
    #[serde(rename = "fullPath")]
    full_path: String,
    #[serde(rename = "firstPrompt")]
    first_prompt: Option<String>,
    summary: Option<String>,
    #[serde(rename = "messageCount")]
    message_count: Option<u32>,
    created: Option<String>,
    modified: Option<String>,
    #[serde(rename = "gitBranch")]
    git_branch: Option<String>,
    #[serde(rename = "projectPath")]
    project_path: Option<String>,
}
```

### Step 3: Implement JSONL token extraction

Parse session JSONL files to sum token usage:
```rust
fn extract_tokens(jsonl_path: &Path) -> TokenSummary {
    // Read file line by line
    // For each line: parse JSON, check if type == "assistant"
    // Sum message.usage.input_tokens, output_tokens, cache_read_input_tokens, cache_creation_input_tokens
    // Extract model from first assistant message
}
```

### Step 4: Write preview() implementation

Show:
- "3 sessions today"
- "125,000 tokens used"
- "Last: v0.2 planning (2h ago)"
- Fields marked sensitive: project paths, session titles

### Step 5: Register in state.rs and sources/mod.rs

Add `pub mod claude_sessions;` and `pub use claude_sessions::ClaudeSessionsSource;`

Register in `state.rs`:
```rust
match ClaudeSessionsSource::new() {
    Ok(source) => {
        tracing::info!("Registered ClaudeSessionsSource");
        source_manager.register(Arc::new(source));
    }
    Err(e) => tracing::warn!("Could not initialize Claude sessions source: {}", e),
}
```

### Step 6: Write tests

- `test_parse_sessions_index` — mock sessions-index.json, verify parsing
- `test_extract_tokens` — mock JSONL with token usage, verify sum
- `test_core_data_timestamp_irrelevant` — this source uses ISO timestamps, not Core Data
- `test_preview_formatting` — verify preview fields
- `test_no_sessions_dir` — graceful fallback when ~/.claude doesn't exist

### Step 7: Run tests and commit

```bash
cargo test claude_sessions
cargo clippy -- -D warnings
git add src-tauri/src/sources/claude_sessions.rs src-tauri/src/sources/mod.rs src-tauri/src/state.rs
git commit -m "feat(sources): add Claude Code Sessions source"
```

---

## Task 5: Apple Sources (Podcasts, Notes, Photos)

**Purpose:** Three new sources for Apple data. Each follows the Source trait pattern.

**These can be implemented in parallel as three independent sub-tasks.**

**Shared dependency:** `rusqlite` already in Cargo.toml with `bundled` feature.

### Task 5a: Apple Podcasts Source

**Files:**
- Create: `src-tauri/src/sources/apple_podcasts.rs`
- Modify: `src-tauri/src/sources/mod.rs`

**Reference:** `docs/research/research-apple-podcasts.md` — contains complete Rust implementation code.

**Key implementation points:**
- Database: `~/Library/Group Containers/243LU875E5.groups.com.apple.podcasts/Documents/MTLibrary.sqlite`
- Open read-only: `SQLITE_OPEN_READ_ONLY`
- Core Data epoch offset: `978307200.0`
- Tables: `ZMTEPISODE` JOIN `ZMTPODCAST`
- Privacy: Episode/podcast titles marked `sensitive: true`
- Requires Full Disk Access

**Source trait:**
- `id()` → `"apple-podcasts"`
- `name()` → `"Apple Podcasts"`
- `watch_path()` → the SQLite database path
- `parse()` → recent episodes + today summary
- `preview()` → episodes today, most recent episode

**Tests:**
- Timestamp conversion
- Mock SQLite database with sample data
- Handle missing database gracefully

### Task 5b: Apple Notes Source

**Files:**
- Create: `src-tauri/src/sources/apple_notes.rs`
- Create: `src-tauri/src/sources/notes.jxa` (JavaScript for Automation script)
- Modify: `src-tauri/src/sources/mod.rs`

**Reference:** `docs/research/research-apple-notes.md`

**Key implementation points:**
- Hybrid approach: watch NoteStore.sqlite, query via JXA
- JXA script returns JSON with note titles, folders, timestamps (NO content)
- Execute via `std::process::Command::new("osascript").arg("-l").arg("JavaScript").arg("-e").arg(SCRIPT)`
- Parse JSON output
- No Full Disk Access required (uses Notes.app permissions via Automation)
- Requires Automation permission (user prompted on first run)

**JXA script (embedded as string):**
```javascript
const Notes = Application('Notes');
const notes = Notes.notes().slice(0, 50).map(note => ({
    title: note.name(),
    created: note.creationDate().toISOString(),
    modified: note.modificationDate().toISOString(),
    folder: note.container().name()
}));
JSON.stringify({ notes: notes, total: Notes.notes().length });
```

**Source trait:**
- `id()` → `"apple-notes"`
- `name()` → `"Apple Notes"`
- `watch_path()` → `~/Library/Group Containers/group.com.apple.notes/NoteStore.sqlite`
- `parse()` → execute JXA, parse JSON, return metadata
- `preview()` → total notes, recent titles (sensitive)

### Task 5c: Apple Photos Source

**Files:**
- Create: `src-tauri/src/sources/apple_photos.rs`
- Modify: `src-tauri/src/sources/mod.rs`

**Reference:** `docs/research/research-apple-photos.md` — contains complete Rust implementation.

**Key implementation points:**
- Database: `~/Pictures/Photos Library.photoslibrary/database/Photos.sqlite`
- Read-only access (safe even while Photos.app running)
- Core Data epoch offset: `978307200`
- Tables: ZASSET, ZGENERICALBUM, Z_26ASSETS (check Z_PRIMARYKEY for actual number)
- Stats only: total photos/videos, albums, recent imports, favorites
- Optional opt-in tiers: scenes, faces, location (all OFF by default)
- Requires Full Disk Access

**Source trait:**
- `id()` → `"apple-photos"`
- `name()` → `"Apple Photos"`
- `watch_path()` → database parent directory
- `parse()` → library stats, recent activity, album stats
- `preview()` → total assets, recent imports, favorites

### Step (all 5a/5b/5c): Register in state.rs

Each source registered with graceful error handling (source may not be available if permissions not granted or app not installed):

```rust
// Register Apple sources (graceful - may fail due to permissions)
for source_result in [
    ApplePodcastsSource::new().map(|s| Arc::new(s) as Arc<dyn Source>),
    AppleNotesSource::new().map(|s| Arc::new(s) as Arc<dyn Source>),
    ApplePhotosSource::new().map(|s| Arc::new(s) as Arc<dyn Source>),
] {
    match source_result {
        Ok(source) => {
            tracing::info!("Registered source: {}", source.name());
            source_manager.register(source);
        }
        Err(e) => tracing::warn!("Optional source unavailable: {}", e),
    }
}
```

### Step: Commit each source independently

```bash
# After each source:
cargo test apple_podcasts  # or apple_notes, apple_photos
cargo clippy -- -D warnings
git add src-tauri/src/sources/apple_*.rs src-tauri/src/sources/mod.rs
git commit -m "feat(sources): add Apple Podcasts source"
```

---

## Task 6: Source-to-Target Binding

**Purpose:** The enable flow — when user enables a source, they select a target system and specific endpoint. This binding is persisted and used by the delivery worker.

**Files:**
- Create: `src-tauri/src/bindings.rs`
- Modify: `src-tauri/src/source_manager.rs` (binding-aware enable/disable)
- Modify: `src-tauri/src/commands/mod.rs` (binding commands)
- Modify: `src-tauri/src/state.rs`

### Step 1: Define Binding model

```rust
/// A binding between a source and a target endpoint
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SourceBinding {
    pub source_id: String,
    pub target_id: String,
    pub endpoint_id: String,
    pub endpoint_url: String,
    pub auth: WebhookAuth,
    pub created_at: i64,
    pub active: bool,
}
```

### Step 2: Create BindingStore

Store bindings in the config SQLite database:

```rust
pub struct BindingStore {
    config: Arc<AppConfig>,
}

impl BindingStore {
    pub fn save_binding(&self, binding: &SourceBinding) -> Result<(), ...> { ... }
    pub fn get_bindings_for_source(&self, source_id: &str) -> Vec<SourceBinding> { ... }
    pub fn remove_binding(&self, source_id: &str, endpoint_id: &str) -> Result<(), ...> { ... }
    pub fn list_all_bindings(&self) -> Vec<SourceBinding> { ... }
}
```

### Step 3: Modify delivery flow

The `handle_file_event` in SourceManager needs to look up bindings for the source and deliver to each bound endpoint instead of the single global webhook URL.

### Step 4: Add Tauri commands

```rust
#[tauri::command]
pub async fn bind_source_to_endpoint(...) -> Result<(), String> { ... }

#[tauri::command]
pub async fn unbind_source_from_endpoint(...) -> Result<(), String> { ... }

#[tauri::command]
pub fn get_source_bindings(...) -> Result<Vec<serde_json::Value>, String> { ... }
```

### Step 5: Write tests and commit

```bash
cargo test binding
git commit -m "feat(bindings): add source-to-target endpoint binding"
```

---

## Task 7: Frontend — Target Setup (Settings Tab)

**Purpose:** Replace the simple webhook URL form with a target management UI. Three-layer: connect n8n/ntfy instance, then browse endpoints in the source enable flow.

**Files:**
- Create: `src/components/TargetSetup.tsx`
- Create: `src/components/N8nConnect.tsx`
- Create: `src/components/NtfyConnect.tsx`
- Modify: `src/components/SettingsPanel.tsx`
- Create: `src/api/hooks/useTargets.ts`

### N8nConnect flow:
1. User enters instance URL (e.g. `https://flow.rightaim.ai`)
2. UI shows link: "Get your API key at {url}/settings/api"
3. User pastes API key
4. On save: call `connect_n8n_target` → test connection → show result (active workflow count)
5. On success: persist to Keychain + config

### NtfyConnect flow:
1. User enters server URL (e.g. `https://ntfy.sh` or self-hosted)
2. User enters topic name
3. Optional: auth token
4. Test connection → show result
5. Persist

### Step: Commit

```bash
git add src/components/TargetSetup.tsx src/components/N8nConnect.tsx src/components/NtfyConnect.tsx
git commit -m "feat(ui): add target setup for n8n and ntfy"
```

---

## Task 8: Frontend — Source Cards with Binding Flow

**Purpose:** Update source list to show all sources with transparency, risk assessment, and enable flow that includes target/endpoint selection + security coaching.

**Files:**
- Modify: `src/components/SourceList.tsx`
- Create: `src/components/SourceCard.tsx`
- Create: `src/components/EndpointPicker.tsx`
- Create: `src/components/SecurityCoaching.tsx`
- Modify: `src/components/TransparencyPreview.tsx`

### SourceCard shows:
- Source name + description
- "Coming soon" badge for unavailable sources (greyed out)
- Real sample data rows (from preview)
- Risk assessment text
- Traffic light indicator (green/yellow/red/grey)

### Enable flow (when user clicks Enable):
1. Show TransparencyPreview with real data + risk assessment
2. User confirms data is OK
3. Show EndpointPicker: select target system → select endpoint
4. Show SecurityCoaching: transport check, auth check, guidance
5. User confirms → source becomes active

### Coming Soon cards (Apple sources initially):
- Show description of what it captures
- Show "Coming Soon" badge
- Greyed out, non-interactive
- Show permission requirements (Full Disk Access notice)

### Step: Commit

```bash
git add src/components/SourceCard.tsx src/components/EndpointPicker.tsx src/components/SecurityCoaching.tsx
git commit -m "feat(ui): add source cards with binding flow and security coaching"
```

---

## Task 9: Frontend — Activity Log Panel

**Purpose:** Real-time reverse-chronological feed showing all delivery activity.

**Files:**
- Create: `src/components/ActivityLog.tsx`
- Create: `src/api/hooks/useActivityLog.ts`
- Modify: `src/App.tsx` (add Activity tab)
- Modify: `src-tauri/src/commands/mod.rs` (add activity log command)

### Activity log entry format:
```
14:23:05  Claude Stats     -> Daily Token Report  ✓ 200 OK
14:22:58  Claude Sessions  -> Daily Token Report  ✓ 200 OK
13:15:02  Claude Stats     -> Daily Token Report  ✗ 503 (retry 1/5)
```

### Click to expand: full payload JSON, response status + body, delivery timing.

### Backend: query delivery_queue + delivery_history tables (already exist in SQLite ledger).

### App.tsx changes:
- Add "Activity" tab between "Sources" and "Settings"
- 4-tab layout: Status | Sources | Activity | Settings

### Step: Commit

```bash
git add src/components/ActivityLog.tsx src/api/hooks/useActivityLog.ts
git commit -m "feat(ui): add activity log panel"
```

---

## Task 10: Traffic Light Indicators + Tray Icons

**Purpose:** Visual status indicators per source and in the system tray.

**Files:**
- Create: `src/components/TrafficLight.tsx`
- Modify: `src/components/SourceCard.tsx` (add traffic light)
- Modify: `src-tauri/src/lib.rs` or tray setup file (tray icon states)

### Traffic Light per source:
| Color | Meaning | Condition |
|-------|---------|-----------|
| Green | Last push delivered | Most recent delivery = Delivered |
| Yellow | Push pending/retrying | Any Pending or InFlight |
| Red | Delivery failed | Any Failed or DLQ |
| Grey | Disabled | Source not enabled |

### Tray Icon states:
| State | Icon | Meaning |
|-------|------|---------|
| No sources enabled | Pause symbol | Idle |
| Any source live | Active arrow | Data being pushed |
| Any source errored | Arrow with badge | Attention needed |

### Backend: add Tauri command to get per-source delivery status for traffic light.

### Step: Commit

```bash
git commit -m "feat(ui): add traffic light indicators and tray icon states"
```

---

## Task 11: Integration + Polish

**Purpose:** Wire everything together, fix cross-cutting issues, update CLAUDE.md.

**Files:**
- Modify: `src-tauri/src/lib.rs` (register all new commands)
- Modify: `CLAUDE.md` (update architecture docs)
- Modify: `src/App.tsx` (final layout)
- Modify: `package.json` (if new npm deps needed)

### Checklist:
- [ ] All new Tauri commands registered in `lib.rs` handler chain
- [ ] All sources registered in `state.rs` with graceful fallback
- [ ] All targets registered on startup from persisted config
- [ ] Delivery worker uses bindings instead of single webhook URL
- [ ] Frontend type definitions match backend structs
- [ ] `npm run check` passes (lint + typecheck + test)
- [ ] `cargo test` passes all tests
- [ ] `cargo clippy -- -D warnings` clean
- [ ] Manual smoke test: `npm run tauri dev`

### Step: Final commit

```bash
git add -A
git commit -m "feat(v0.2): integrate multi-source platform with n8n and ntfy targets"
```

---

## Parallel Execution Strategy

For maximum speed, dispatch these groups in parallel:

**Phase 1 (Backend Foundation):**
- Agent A: Tasks 1-2 (Target trait + ntfy) — sequential, ~20 min
- Agent B: Task 4 (Claude Sessions source) — independent, ~20 min
- Agent C: Task 5a (Apple Podcasts) — independent, ~15 min
- Agent D: Task 5b (Apple Notes) — independent, ~15 min
- Agent E: Task 5c (Apple Photos) — independent, ~15 min

**Phase 2 (Backend Integration):**
- Agent F: Task 3 (n8n target) — depends on Task 1
- Agent G: Task 6 (Bindings) — depends on Tasks 1-2

**Phase 3 (Frontend):**
- Agent H: Tasks 7-8 (Target setup + Source cards) — depends on backend
- Agent I: Task 9 (Activity log) — depends on backend
- Agent J: Task 10 (Traffic lights + tray) — mostly independent

**Phase 4 (Integration):**
- Single agent: Task 11 — verify + polish

---

## Risk Assessment

| Risk | Mitigation |
|------|------------|
| Apple DB schema mismatch | Use `.ok()` for optional columns, schema detection |
| TCC permission denied | Graceful error messages, detection before parse |
| n8n API pagination edge cases | Follow cursor pagination, test with many workflows |
| ntfy server unreachable | Health check before save, retry on failure |
| Cross-file type mismatches (parallel agents) | Budget 2-3 fix rounds after parallel dispatch |
| Frontend-backend contract drift | Generate types from Rust structs where possible |

---

## New Dependencies

**No new Cargo.toml dependencies needed** — all required crates already present:
- `rusqlite` (bundled) — Apple SQLite databases
- `reqwest` (json) — n8n API + ntfy HTTP
- `chrono` (serde) — timestamp handling
- `keyring` — credential storage
- `serde_json` — payload serialization
- `async-trait` — async trait methods

**Note:** The `osascript` crate is NOT needed — we use `std::process::Command` directly for JXA execution, keeping dependencies minimal.
